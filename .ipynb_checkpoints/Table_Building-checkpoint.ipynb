{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5dd33c-ca80-438d-8b3d-bdf65a27e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74d0ed9-077b-495a-bca2-cb0b61fc1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['artist', 'title', 'verse_num', 'line_num', 'token_num']\n",
    "songs = [song for song in sorted(glob('data/*.txt'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4d960-afee-4636-b230-e16391efb716",
   "metadata": {},
   "source": [
    "## Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "28438e30-01dd-4cb6-9d12-031aca456955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLIBDOC(songs, OHCO=OHCO):\n",
    "    lib = []\n",
    "    doc = []\n",
    "    \n",
    "    for song in songs:\n",
    "        \n",
    "        # get artist and title\n",
    "        artist, title = song[5:-4].split('_')\n",
    "        \n",
    "        # import song  lyrics\n",
    "        with open(song, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            df = pd.DataFrame(lines, columns=['line'])\n",
    "\n",
    "            # assign verse numbers\n",
    "            verse_stop = df.line.str.match('\\n| ')\n",
    "            verse_num = [i+1 for i in range(df.loc[verse_stop].shape[0])]\n",
    "            df.loc[verse_stop, 'verse_num'] = verse_num\n",
    "            try:\n",
    "                df.verse_num = df.verse_num.bfill().fillna(verse_num[-1:][0]+1)\n",
    "            except:\n",
    "                df.verse_num = 1\n",
    "            df = df.replace('\\n','')\n",
    "            df = df.loc[-(df.line == '')]\n",
    "            df['line'] = df['line'].str.strip()\n",
    "\n",
    "            df['line'] = df['line'].apply(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x))\n",
    "\n",
    "            # group together, assign new index\n",
    "            df = df.groupby(OHCO[2:3]).line.apply(lambda x: '\\n'.join(x)).to_frame()\n",
    "            df['title'] = title  \n",
    "            df['artist'] = artist  \n",
    "            df = df.reset_index().set_index(OHCO[:3]).rename(columns={'line':'verse'})\n",
    "\n",
    "            lib.append((artist, title, song))\n",
    "            doc.append(df)\n",
    "        \n",
    "    DOC = pd.concat(doc)\n",
    "    LIB = pd.DataFrame(lib, columns=['artist', 'title', 'song_file']).set_index('artist')\n",
    "    return LIB, DOC\n",
    "\n",
    "def buildTOKEN(doc, OHCO=OHCO):\n",
    "    \n",
    "    # Convert verses to lines\n",
    "    df = doc.verse.apply(lambda x: pd.Series(x.split('\\n'))).stack().to_frame()\n",
    "    df = df.rename(columns={0:'line'})\n",
    "    \n",
    "    # Convert sentences to tokens\n",
    "    df = df.line.apply(lambda x: pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))).stack().to_frame()\n",
    "    df = df.rename(columns={0:'temp'})\n",
    "    df['token'] = df['temp'].apply(lambda x: x[0])\n",
    "    df['pos'] = df['temp'].apply(lambda x: x[1])\n",
    "    \n",
    "    df.index.names = OHCO\n",
    "    \n",
    "    return df\n",
    "\n",
    "def buildVOCAB(token):\n",
    "    \n",
    "    # get count of each token\n",
    "    df = token['token'].value_counts().to_frame().rename(columns={'index':'token', 'token':'count'})\n",
    "    \n",
    "    # sort tokens by alphabetic order\n",
    "    df = df.sort_index().reset_index().rename(columns={'index':'token'})\n",
    "    \n",
    "    # assign alphabetic order as ID\n",
    "    df.index.name = 'token_id'\n",
    "    \n",
    "    # identify numbers\n",
    "    df['num'] = df['token'].str.match(\"\\d+\").astype('int')\n",
    "    \n",
    "    # add stop word flag\n",
    "    stopwords = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['token']) \\\n",
    "        .set_index('token').rename(columns={'index':'temp'})\n",
    "    stopwords['temp'] = 1\n",
    "    df['stop'] = df['token'].map(stopwords['temp']).fillna(0).astype('int')\n",
    "\n",
    "    # add token stems\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    df['p_stem'] = df['token'].apply(stemmer.stem)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def updateTOKENVOCAB(token, vocab):\n",
    "    \n",
    "    # add token id to TOKEN\n",
    "    token['token_id'] = token['token'].map(vocab.reset_index().set_index('token')['token_id'])\n",
    "    \n",
    "    # add most frequent POS to VOCAB\n",
    "    vocab['pos_max'] = token.groupby(['token_id', 'pos'])['pos'].count().unstack().idxmax(1)\n",
    "    return token, vocab    \n",
    "\n",
    "def buildZIPF(vocab):\n",
    "    \n",
    "    # add count rank\n",
    "    vocab = vocab.sort_values('count', ascending=False).reset_index()\n",
    "    vocab['count_rank'] = vocab.reset_index().index + 1\n",
    "    vocab = vocab.set_index('token_id')\n",
    "    \n",
    "    # add capped count rank\n",
    "    alt_rank = vocab['count'].value_counts().sort_index(ascending=False) \\\n",
    "                .reset_index().reset_index() \\\n",
    "                .rename(columns={'level_0':'alt_rank', 'index':'count', 'count':'nn'}) \\\n",
    "                .set_index('count')\n",
    "    vocab['alt_rank'] = vocab['count'].map(alt_rank['alt_rank']) + 1\n",
    "    \n",
    "    # straight probability of seeing the token\n",
    "    vocab['p'] = vocab['count'] / vocab.shape[0]\n",
    "    \n",
    "    # marginal probability of seeing the token\n",
    "    vocab['p2'] = vocab['count'] / vocab['count'].sum()\n",
    "\n",
    "    # calculate Zipf values\n",
    "    vocab['zipf_k'] = vocab['count'] * vocab['count_rank']\n",
    "    vocab['zipf_k2'] = vocab['count'] * vocab['alt_rank']\n",
    "    vocab['zipf_k3'] = vocab['p'] * vocab['alt_rank']\n",
    "    \n",
    "    # assign the self entropy of each token\n",
    "    vocab['h'] = vocab['p2'] * np.log2(1/vocab['p2'])\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def buildTFIDF(token, vocab, bag, count_type, term_freq_type, inverse_doc_type):\n",
    "    bag = bag\n",
    "    count_method = count_type\n",
    "    tf_method = term_freq_type\n",
    "    idf_method = inverse_doc_type\n",
    "\n",
    "    # generate bag of words at set bag level\n",
    "    BOW = token.groupby(bag+['token_id'])['token_id'].count().to_frame().rename(columns={'token_id':'n'})\n",
    "    BOW['c'] = BOW.n.astype('bool').astype('int')\n",
    "    \n",
    "    # create document term count matrix\n",
    "    DTCM = BOW[count_method].unstack().fillna(0).astype('int')\n",
    "    \n",
    "    # compute TF and transpose\n",
    "    if tf_method == 'sum': TF = DTCM.T / DTCM.T.sum()\n",
    "    elif tf_method == 'max': TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'log': TF = np.log10(1 + DTCM.T)\n",
    "    elif tf_method == 'raw': TF = DTCM.T\n",
    "    elif tf_method == 'double_norm':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "        TF = 0.5 + (1 - 0.5) * TF[TF > 0]\n",
    "    elif tf_method == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')\n",
    "    TF = TF.T\n",
    "        \n",
    "    # compute DF and IDF\n",
    "    DF = DTCM[DTCM > 0].count()\n",
    "    N = DTCM.shape[0]\n",
    "    \n",
    "    # idf method selection\n",
    "    if idf_method == 'standard': IDF = np.log10(N / DF)\n",
    "    elif idf_method == 'max': IDF = np.log10(DF.max() / DF) \n",
    "    elif idf_method == 'smooth': IDF = np.log10((1 + N) / (1 + DF)) + 1\n",
    "    \n",
    "    # compute word context matrix entropy\n",
    "    WCM = DTCM / DTCM.sum()\n",
    "    WCMh = WCM * np.log2(1/WCM)\n",
    "    \n",
    "    # compute TFIDF\n",
    "    TFIDF = TF * IDF\n",
    "    \n",
    "    # assign values to the VOCAB df\n",
    "    vocab['h2'] = WCMh.sum()\n",
    "    vocab['DF'] = DF\n",
    "    vocab['IDF'] = IDF\n",
    "    vocab['TFIDF_sum'] = TFIDF.sum()\n",
    "\n",
    "    return TFIDF, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e294258-0641-4f96-9fc3-0096f24325b4",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e13e97d4-1f85-4585-83a7-02cbf66f1f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cschr\\AppData\\Local\\Temp/ipykernel_23036/3670687945.py:49: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df = df.line.apply(lambda x: pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))).stack().to_frame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LIB, DOC = buildLIBDOC(songs)\n",
    "TOKEN = buildTOKEN(DOC)\n",
    "VOCAB = buildVOCAB(TOKEN)\n",
    "TOKEN, VOCAB = updateTOKENVOCAB(TOKEN, VOCAB)\n",
    "VOCAB = buildZIPF(VOCAB)\n",
    "TFIDF, VOCAB = buildTFIDF(TOKEN, VOCAB, OHCO[:2], 'n', 'sum', 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394f2de-a76b-4bae-890d-bef1ed26090b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
