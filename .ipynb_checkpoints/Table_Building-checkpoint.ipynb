{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35470c85-0984-42b2-bb09-2661238f1d37",
   "metadata": {},
   "source": [
    "Christian Schroeder (dbn5eu@virginia.edu)\n",
    "\n",
    "DS 5001: Exploratory Text Analytics\n",
    "\n",
    "15 December 2021\n",
    "\n",
    "# Build Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5dd33c-ca80-438d-8b3d-bdf65a27e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cschr\\anaconda3\\envs\\pythonenv-nlp\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4d960-afee-4636-b230-e16391efb716",
   "metadata": {},
   "source": [
    "## Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28438e30-01dd-4cb6-9d12-031aca456955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index names for the data\n",
    "OHCO = ['artist', 'song_id', 'verse_num', 'line_num', 'token_num']\n",
    "\n",
    "# build the LIB and DOC tables\n",
    "def buildLIBDOC(songs, OHCO=OHCO):\n",
    "    \n",
    "    lib = []\n",
    "    doc = []\n",
    "    \n",
    "    for i, song in songs.iterrows():\n",
    "        \n",
    "        artist = song['artist']\n",
    "        title = song['title']\n",
    "        song_id = i\n",
    "        \n",
    "        verse_list = []\n",
    "        verses = song['lyrics'].split('\\n\\n')\n",
    "        for i in range(len(verses)):\n",
    "            verse_list.append([artist, song_id, i+1, verses[i]])\n",
    "        df = pd.DataFrame(verse_list, columns=['artist','song_id','verse_num','verse']).set_index(OHCO[:3])\n",
    "            \n",
    "        lib.append([artist, song_id, title])\n",
    "        doc.append(df)\n",
    "    \n",
    "    DOC = pd.concat(doc)\n",
    "    LIB = pd.DataFrame(lib, columns=['artist', 'song_id', 'title']).set_index('artist')\n",
    "    return LIB, DOC\n",
    "        \n",
    "    DOC = pd.concat(doc)\n",
    "    LIB = pd.DataFrame(lib, columns=['artist', 'song_id', 'title', 'song_file']).set_index('artist')\n",
    "    return LIB, DOC\n",
    "\n",
    "# build the TOKEN table\n",
    "def buildTOKEN(doc, OHCO=OHCO):\n",
    "    \n",
    "    # Convert verses to lines\n",
    "    df = doc.verse.apply(lambda x: pd.Series(x.split('\\n'))).stack().to_frame()\n",
    "    df = df.rename(columns={0:'line'})\n",
    "    df['line'] = df['line'].apply(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x))\n",
    "    \n",
    "    # Convert sentences to tokens\n",
    "    df = df.line.apply(lambda x: pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))).stack().to_frame()\n",
    "    df = df.rename(columns={0:'temp'})\n",
    "    df['token'] = df['temp'].apply(lambda x: x[0])\n",
    "    df['pos'] = df['temp'].apply(lambda x: x[1])\n",
    "    \n",
    "    df.index.names = OHCO\n",
    "    \n",
    "    return df\n",
    "\n",
    "# build the VOCAB table\n",
    "def buildVOCAB(token):\n",
    "    \n",
    "    # get count of each token\n",
    "    df = token['token'].value_counts().to_frame().rename(columns={'index':'token', 'token':'count'})\n",
    "    \n",
    "    # sort tokens by alphabetic order\n",
    "    df = df.sort_index().reset_index().rename(columns={'index':'token'})\n",
    "    \n",
    "    # assign alphabetic order as ID\n",
    "    df.index.name = 'token_id'\n",
    "    \n",
    "    # identify numbers\n",
    "    df['num'] = df['token'].str.match(\"\\d+\").astype('int')\n",
    "    \n",
    "    # add stop word flag\n",
    "    stopwords = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['token']) \\\n",
    "        .set_index('token').rename(columns={'index':'temp'})\n",
    "    stopwords['temp'] = 1\n",
    "    df['stop'] = df['token'].map(stopwords['temp']).fillna(0).astype('int')\n",
    "\n",
    "    # add token stems\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    df['p_stem'] = df['token'].apply(stemmer.stem)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# update TOKEN and VOCAB with the token ids and selected part of speech\n",
    "def updateTOKENVOCAB(token, vocab):\n",
    "    \n",
    "    # add token id to TOKEN\n",
    "    token['token_id'] = token['token'].map(vocab.reset_index().set_index('token')['token_id'])\n",
    "    \n",
    "    # add most frequent POS to VOCAB\n",
    "    vocab['pos_max'] = token.groupby(['token_id', 'pos'])['pos'].count().unstack().idxmax(1)\n",
    "    return token, vocab    \n",
    "\n",
    "# calculate Zipf values and assign to the VOCAB table\n",
    "def buildZIPF(vocab):\n",
    "    \n",
    "    # add count rank\n",
    "    vocab = vocab.sort_values('count', ascending=False).reset_index()\n",
    "    vocab['count_rank'] = vocab.reset_index().index + 1\n",
    "    vocab = vocab.set_index('token_id')\n",
    "    \n",
    "    # add capped count rank\n",
    "    alt_rank = vocab['count'].value_counts().sort_index(ascending=False) \\\n",
    "                .reset_index().reset_index() \\\n",
    "                .rename(columns={'level_0':'alt_rank', 'index':'count', 'count':'nn'}) \\\n",
    "                .set_index('count')\n",
    "    vocab['alt_rank'] = vocab['count'].map(alt_rank['alt_rank']) + 1\n",
    "    \n",
    "    # straight probability of seeing the token\n",
    "    vocab['p'] = vocab['count'] / vocab.shape[0]\n",
    "    \n",
    "    # marginal probability of seeing the token\n",
    "    vocab['p2'] = vocab['count'] / vocab['count'].sum()\n",
    "\n",
    "    # calculate Zipf values\n",
    "    vocab['zipf_k'] = vocab['count'] * vocab['count_rank']\n",
    "    vocab['zipf_k2'] = vocab['count'] * vocab['alt_rank']\n",
    "    vocab['zipf_k3'] = vocab['p'] * vocab['alt_rank']\n",
    "    \n",
    "    # assign the self entropy of each token\n",
    "    vocab['h'] = vocab['p2'] * np.log2(1/vocab['p2'])\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# build the TFIDF table and update VOCAB\n",
    "def buildTFIDF(token, vocab, bag, count_type, term_freq_type, inverse_doc_type):\n",
    "    bag = bag\n",
    "    count_method = count_type\n",
    "    tf_method = term_freq_type\n",
    "    idf_method = inverse_doc_type\n",
    "\n",
    "    # generate bag of words at set bag level\n",
    "    BOW = token.groupby(bag+['token_id'])['token_id'].count().to_frame().rename(columns={'token_id':'n'})\n",
    "    BOW['c'] = BOW.n.astype('bool').astype('int')\n",
    "    \n",
    "    # create document term count matrix\n",
    "    DTCM = BOW[count_method].unstack().fillna(0).astype('int')\n",
    "    \n",
    "    # compute TF and transpose\n",
    "    if tf_method == 'sum': TF = DTCM.T / DTCM.T.sum()\n",
    "    elif tf_method == 'max': TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'log': TF = np.log10(1 + DTCM.T)\n",
    "    elif tf_method == 'raw': TF = DTCM.T\n",
    "    elif tf_method == 'double_norm':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "        TF = 0.5 + (1 - 0.5) * TF[TF > 0]\n",
    "    elif tf_method == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')\n",
    "    TF = TF.T\n",
    "        \n",
    "    # compute DF and IDF\n",
    "    DF = DTCM[DTCM > 0].count()\n",
    "    N = DTCM.shape[0]\n",
    "    \n",
    "    # idf method selection\n",
    "    if idf_method == 'standard': IDF = np.log10(N / DF)\n",
    "    elif idf_method == 'max': IDF = np.log10(DF.max() / DF) \n",
    "    elif idf_method == 'smooth': IDF = np.log10((1 + N) / (1 + DF)) + 1\n",
    "    \n",
    "    # compute word context matrix entropy\n",
    "    WCM = DTCM / DTCM.sum()\n",
    "    WCMh = WCM * np.log2(1/WCM)\n",
    "    \n",
    "    # compute TFIDF\n",
    "    TFIDF = TF * IDF\n",
    "    \n",
    "    # assign values to the VOCAB df\n",
    "    vocab['h2'] = WCMh.sum()\n",
    "    vocab['DF'] = DF\n",
    "    vocab['IDF'] = IDF\n",
    "    vocab['TFIDF_sum'] = TFIDF.sum()\n",
    "\n",
    "    return TFIDF, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e294258-0641-4f96-9fc3-0096f24325b4",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d07ed609-77df-4014-8a30-658285d23781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all table building\n",
    "def pipeline(create_new = True):\n",
    "    \n",
    "    if create_new:\n",
    "        \n",
    "        # run the pipeline\n",
    "        LIB, DOC = buildLIBDOC(archive)\n",
    "        TOKEN = buildTOKEN(DOC)\n",
    "        VOCAB = buildVOCAB(TOKEN)\n",
    "        TOKEN, VOCAB = updateTOKENVOCAB(TOKEN, VOCAB)\n",
    "        VOCAB = buildZIPF(VOCAB)\n",
    "        TFIDF, VOCAB = buildTFIDF(TOKEN, VOCAB, OHCO[:1], 'n', 'sum', 'standard')\n",
    "        \n",
    "        # export\n",
    "        LIB.to_csv('data/tables/LIB.csv')\n",
    "        DOC.to_csv('data/tables/DOC.csv')\n",
    "        TOKEN[:int(TOKEN.shape[0]/2)].to_csv('data/tables/TOKEN1.csv')\n",
    "        TOKEN[int(TOKEN.shape[0]/2):].to_csv('data/tables/TOKEN2.csv')\n",
    "        VOCAB.to_csv('data/tables/VOCAB.csv')\n",
    "        TFIDF.to_csv('data/tables/TFIDF.csv')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # import previously created tables\n",
    "        LIB = pd.read_csv('data/tables/LIB.csv')\n",
    "        DOC = pd.read_csv('data/tables/DOC.csv').set_index(OHCO[:3])\n",
    "        TOKEN = pd.concat([pd.read_csv('data/tables/TOKEN1.csv').set_index(OHCO),pd.read_csv('data/tables/TOKEN2.csv').set_index(OHCO)])\n",
    "        VOCAB = pd.read_csv('data/tables/VOCAB.csv').set_index('token_id')\n",
    "        TFIDF = pd.read_csv('data/tables/TFIDF.csv').set_index('artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec6fe8-fd86-44c8-93d7-3a069a3b756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import song lyrics dataset\n",
    "archive = pd.read_csv('data/songs_archive.csv', dtype={'lyrics':'string'}).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# run the pipeline to build the tables\n",
    "pipeline(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fe6e6e7-6065-46c8-b431-e65097520a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aesop Rock', 'Busdriver', 'Jedi Mind Tricks', 'GZA',\n",
       "       'Wu-Tang Clan', 'MF DOOM', 'RZA', 'Immortal Technique', 'Canibus',\n",
       "       'Ghostface Killah', 'Del The Funky Homosapien', 'The Roots',\n",
       "       'Blackalicious', 'Jean Grae', 'Killah Priest', 'Kool Keith',\n",
       "       'Kool G Rap', 'CunninLynguists', 'Sage Francis', 'Raekwon',\n",
       "       'Watsky', 'Action Bronson', 'Redman', 'Das EFX', 'Common',\n",
       "       'K.A.A.N.', 'E-40', 'Goodie Mob', 'Nas', 'Brother Ali',\n",
       "       'Method Man', 'Flatbush Zombies', 'Joey Badass',\n",
       "       'A Tribe Called Quest', 'Yasiin Bey', 'De La Soul', 'Xzibit',\n",
       "       'Murs', 'Rittz', 'Atmosphere', 'Talib Kweli', 'Big Daddy Kane',\n",
       "       'Lupe Fiasco', 'Cypress Hill', 'LL Cool J', 'Beastie Boys',\n",
       "       'Fat Joe', 'K-Rino', 'Busta Rhymes', 'Gang Starr', 'Mac Dre',\n",
       "       'Ludacris', 'KRS-One', 'OutKast', 'Brand Nubian', 'Ab-Soul',\n",
       "       'Joe Budden', 'Twista', 'Eminem', 'Tyler, The Creator',\n",
       "       'Denzel Curry', 'Biz Markie', 'AsAP Rocky', 'Tech N9ne',\n",
       "       'Royce da 5\\'9\"', 'SUICIDEBOYS', 'Geto Boys', 'Danny Brown',\n",
       "       'Ice Cube', 'Ice-T', 'Diddy', 'Kevin Gates', 'Death Grips',\n",
       "       'JAY-Z', 'Wale', 'Public Enemy', 'Mobb Deep', 'Cam’ron',\n",
       "       'The Game', 'Jadakiss', 'T.I.', 'Jay Rock', 'Lil’ Kim', 'Kano',\n",
       "       'ScHoolboy Q', 'Nelly', 'Kendrick Lamar', 'Big L', 'Three 6 Mafia',\n",
       "       '\\u200bcupcakKe', 'Mac Miller', 'Tyga', 'Skyzoo', 'BROCKHAMPTON',\n",
       "       'Rick Ross', 'Insane Clown Posse', 'Scarface', 'Run–D.M.C.',\n",
       "       'Hopsin', 'Vince Staples', 'Bun B', 'UGK', 'Dizzee Rascal',\n",
       "       'AsAP Ferg', 'Lil Peep', 'Big K.R.I.T.', 'MC Lyte', '2Pac',\n",
       "       '2 Chainz', 'Snoop Dogg', 'Foxy Brown', 'Kanye West', 'B.o.B',\n",
       "       'Machine Gun Kelly', 'Lecrae', 'Childish Gambino', 'Juelz Santana',\n",
       "       'Eve', 'Nicki Minaj', 'Salt-N-Pepa', 'Master P', 'Big Sean',\n",
       "       'Trina', 'Gucci Mane', 'Russ', 'Lil Wayne', 'Jeezy', 'G-Eazy',\n",
       "       'Meek Mill', 'Trick Daddy', 'Juvenile', 'J. Cole', 'Young Thug',\n",
       "       'Travis Scott', '50 Cent', 'Drake', 'Future', 'Kid Cudi', 'Logic',\n",
       "       'Juicy J', 'Bone Thugs-N-Harmony', 'Kodak Black', 'Migos',\n",
       "       'Lil Yachty', 'Kid Ink', 'Wiz Khalifa', 'YG', 'Lil Durk',\n",
       "       '21 Savage', 'DMX', 'Rich Homie Quan', 'Too short', 'Lil Baby',\n",
       "       'A Boogie wit da Hoodie', 'YoungBoy Never Broke Again',\n",
       "       'Rich The Kid', 'Lil Uzi Vert', 'NF'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC.reset_index().artist.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f15f83c7-619a-4cc1-9735-0e2f48877fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = pd.read_csv('data/lyric_award_rankings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8fdf28d-3c45-4ec6-af36-8e654f024317",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists['artist2'] = pd.Series(DOC.reset_index().artist.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01bcc7b8-9e06-4721-8fe7-73399d8274f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = artists.merge(pd.DataFrame(pd.Series(DOC.reset_index().artist.unique())).rename(columns={0:'artist'}), on='artist', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1dc51e3-dd24-4bc6-bcab-7867c54d999c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrical_rank</th>\n",
       "      <th>recalc</th>\n",
       "      <th>wins</th>\n",
       "      <th>noms</th>\n",
       "      <th>win_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Yasiin Bey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Tyler, The Creator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Cam’ron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Lil’ Kim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>​cupcakKe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Lil Peep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 artist  lyrical_rank  recalc  wins  noms  win_rate\n",
       "160          Yasiin Bey           NaN     NaN   NaN   NaN       NaN\n",
       "161  Tyler, The Creator           NaN     NaN   NaN   NaN       NaN\n",
       "162             Cam’ron           NaN     NaN   NaN   NaN       NaN\n",
       "163            Lil’ Kim           NaN     NaN   NaN   NaN       NaN\n",
       "164           ​cupcakKe           NaN     NaN   NaN   NaN       NaN\n",
       "165            Lil Peep           NaN     NaN   NaN   NaN       NaN"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[test.noms.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53df7227-8d9b-41d2-ba79-ed6ecce8ebc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aesop Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Busdriver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jedi Mind Tricks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wu-Tang Clan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>A Boogie wit da Hoodie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>YoungBoy Never Broke Again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Rich The Kid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>NF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         artist\n",
       "0                    Aesop Rock\n",
       "1                     Busdriver\n",
       "2              Jedi Mind Tricks\n",
       "3                           GZA\n",
       "4                  Wu-Tang Clan\n",
       "..                          ...\n",
       "153      A Boogie wit da Hoodie\n",
       "154  YoungBoy Never Broke Again\n",
       "155                Rich The Kid\n",
       "156                Lil Uzi Vert\n",
       "157                          NF\n",
       "\n",
       "[158 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f5580-9c26-4b49-beb3-51b674d04640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
